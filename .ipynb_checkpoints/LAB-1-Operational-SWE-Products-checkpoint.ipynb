{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operational Snow Water Equivalent (SWE) Products \n",
    "In this lab we will explore two operational SWE products to estimate basin wide SWE. These are standard products, produced in near real time (NRT), that are publically avalible. A word of caution: as you might remember from the lectures, estimating SWE in the mountains is hard. Operational SWE products perform better in large, flat areas with consisten depth and densisty of snowpack across many square kilometeres. Fortunatly, ongoing development of these products means they are continuously improving. While these products might not produce completely accurate basin scale SWE, they can often be used as in index along with data from previous years. \n",
    "\n",
    "## Two types of SWE products: Measurements and Models\n",
    "There are two main catagories of operational SWE products: remotely sensed Measurements of SWE and process based model estimates of SWE. We will explore an example of both. For Measurement of SWE, we will look at data from the unified Advanced Microwave Scanning Radiometer (AMSR) SWE products which provide daily estimates of global SWE. Note that the product may not be available in some mountainous regions. For process based Model estimates of SWE we will explore the newest near real-time (NRT) ECWMF Re-analysis (ERA-5) SWE estimates. \n",
    "\n",
    "Goals of the lab:\n",
    "- Gain familiarity with SWE plots\n",
    "- Understand the two main sources of SWE estimates: remotely sensed measurements and process based models\n",
    "- Gain familiarty with spatial interpolation of data\n",
    "- Data science techniques to visualize the SWE in the context of historical records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: AMSR-Unified SWE estimates\n",
    "\n",
    "The first product we are going to look at is the AMSR Unifed SWE product. These data are collected from satellites that measure the emission of microwave radiation from Earth. Snow scatters and absorbs the microwave radiation.  Based on the measured reduction in passive microwave signal, we can estimate the snow water equivalent. This method works best in areas that have shallow snowpack, are devoid of forests, and are flat for at least 25 kilometers.\n",
    "\n",
    "This AMSR-E Unified SWE product is produced using data collected by the Advanced Microwave Scanning Radiometer (AMSR-E) on board the Aqua satellite from June 2002 to October 2011, and from the Advanced Microwave Scanning Radiometer 2 (AMSR2) on board the JAXA GCOM-W1 satellite from July 2012 to the present. The purpose of the AMSR Unified Data Set is to provide the science community with intercalibrated climate products from both the AMSR-E and the AMSR2 instruments. \n",
    "\n",
    "The data sets are archived here:\n",
    "\n",
    "AMSR-E SWE: 2002-2011\n",
    "https://nsidc.org/data/AE_DySno/versions/2\n",
    "\n",
    "\n",
    "AMSR2 SWE: 2012-now \n",
    "https://nsidc.org/data/AU_DySno/versions/1\n",
    "\n",
    "\n",
    "These data are avalible as daily, 5-day maximum, and monthly maximum datasets. We will look at the daily data in this lab exercise. \n",
    "\n",
    "## Set up the Lab workspace\n",
    "To explore these datasets we need to import the required packages for geospatial analysis in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd \n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import os\n",
    "import contextily as ctx\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an Area of Interest\n",
    "We have three different AOIs. Choose one of the following to complete your lab: Langtang catchment in Nepal, Panjsher basin in Afganistan, or wangchu watershed in Bhutan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at the Area of Interest (AOI) on a map. Pick one of the following:\n",
    "\n",
    "#choose on of the following areas for your labratory. \n",
    "#Feel free to explore each of them!\n",
    "#panjsher_basin_afghanistan\n",
    "#wangchu_watershed_bhutan\n",
    "#langtang_catchment_nepal\n",
    "############## INSERT AOI WATERSHED NAME HERE ###################\n",
    "watershedName='panjsher_basin_afghanistan'\n",
    "#################################################################\n",
    "aoiFile=watershedName+'_wgs84.shp'\n",
    "basinMaskFile=watershedName+'_basinMask.tif'\n",
    "aoi_AMSR_SWE=watershedName+'_AMSR_SWE.tif'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Area of Interest on a basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load the shapefile with geopandas and display on a basemap with contextily\n",
    "#The 'EPSG' codes are standerdized information that discribe which projection the datasets \n",
    "#are in. We want to add basemap in same projection as the SWE data and AOI data\n",
    "\n",
    "#load\n",
    "AOI = gpd.read_file(aoiFile)\n",
    "AOI.crs= \"EPSG:4326\" #({'init': 'epsg:4326'})\n",
    "\n",
    "#plot\n",
    "ax = AOI.plot(edgecolor=\"purple\", facecolor=\"None\")\n",
    "ctx.add_basemap(ax, crs=\"EPSG:4326\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the AMSR-E SWE data\n",
    "\n",
    "First, it is importnat to see how the data are packaged. The datasets are packaged for the entire northern hemisphere as a single image each day in the daily dataset. This is not very useful for looking a small basin over many days or months! Lets see what the full dataset look like for one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#february 18, 2010 Northerm hemisphere AMSR product.\n",
    "#- FIX: the AMSR-2 data is .he5 not .hdf \n",
    "#print one day of the full AMSR-e data. need to make still."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interesting? This is a unique projection. More information is here:\n",
    "https://nsidc.org/ease/ease-grid-projection-gt\n",
    "\n",
    "add some more information about the data storage format for student info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AMSR SWE for our Area of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reprojected, cropped, and assembled data cubes of the full AMSR Unifed SWE records for this lab for each area of interest. \n",
    "\n",
    "A data cube is a stack of data, for Earth science datasets, one common type of datacube is a \"time/space\" cube. This is when we have data for a specific area, for many days, all assembled into a single file for analysis. For this lab we will be working with time/space datacubs of AMSR SWE data. \n",
    "\n",
    "To start, we will load the datacube, and visualize a few days to get a sense of the SWE data that we have to work with. \n",
    "## Load SWE datacube and check size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load the basin mask\n",
    "bMask=rio.open(basinMaskFile)\n",
    "basinMask = bMask.read()\n",
    "basinMask=np.squeeze(basinMask)\n",
    "\n",
    "#load the SWE dataset\n",
    "aoi_AMSR_SWE=watershedName+'_AMSR_SWE.tif'\n",
    "SWE=rio.open(aoi_AMSR_SWE)\n",
    "SWEarray = SWE.read()\n",
    "numel=SWEarray.shape\n",
    "\n",
    "\n",
    "print(numel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have loaded the SWE datacube for our study area. the ***'.shape'*** command lets us see the size of our datacube. From this, can you figure out how many days of data are in the datacube? what about how many measurements per day?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot SWE at one location for entire data record\n",
    "Next, we will load up the dates for these data, which are stored seperatly and plot one of the pixels in the datacube for the whole record of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dateFile=watershedName+'AMSR_sweDates.csv'\n",
    "SWEdf=pd.read_csv(dateFile,header=None,names=['sweDates'])\n",
    "SWEdf['SWE_mm']=pd.DataFrame(SWEarray[:,4,5])#,columns=['SWE_mm'],index=)\n",
    "SWEdf['sweDates']=pd.to_datetime(SWEdf['sweDates'],format='%Y%m%d')\n",
    "SWEdf=SWEdf.set_index('sweDates')\n",
    "\n",
    "#values above 240 are various flags for no data, we will set to NaN\n",
    "SWEdf[SWEdf>240]=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data organized into a DataFrame, lets plot one of the pixels in the datacube for the whole record of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create figure and plot space\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Add x-axis and y-axis\n",
    "ax.plot(SWEdf.index.values,\n",
    "        SWEdf['SWE_mm'],\n",
    "        color='purple')\n",
    "\n",
    "# Set title and labels for axes\n",
    "ax.set(xlabel=\"Date\",\n",
    "       ylabel=\"Snow Water Equivalent (mm)\",\n",
    "       title=\"Daily Snow Water Equivalent \\n\" + watershedName)\n",
    "\n",
    "# Rotate tick marks on x-axis\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore one water year of data\n",
    "Often we would like to see how snow accumulates and melts over a year. Lets look closey at one water year of data, the 2004-2005 water year which begins October 1, 2004 and ends September 30, 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WY_04_05=SWEdf['2004-10-01':'2005-09-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create figure and plot space\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Add x-axis and y-axis\n",
    "ax.plot(WY_04_05.index.values,\n",
    "        WY_04_05,\n",
    "        color='purple')\n",
    "\n",
    "# Set title and labels for axes\n",
    "ax.set(xlabel=\"Date\",\n",
    "       ylabel=\"Snow Water Equivalent (mm)\",\n",
    "       title=\"Daily Snow Water Equivalent \\n\" + watershedName)\n",
    "\n",
    "# Rotate tick marks on x-axis\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anything look weird? Why does the SWE disapear on lots of days within the data series?\n",
    "\n",
    "\n",
    "This is because the satellite does not take a measurement everyday. Sometimes the pixel is not imaged by the satellite, so there are fill values in the datasets on those days. What can we do about this? One solution is to interpolate the value based on measurements on the preceeding days and following days that are valid measurements. This task is very common with satellite snow observations. Many times there are numerious days where we just do not have a good measurement. This can be becuase of cloud cover, no image was taken on that day, or the image is of poor quality for a variety of reasons. (off nadir satellite view angle, noise, etc....)\n",
    "\n",
    "## Fill and smooth AMSR SWE datacube\n",
    "Now we will smooth the data to remove inconsistencies. This is an example of a simple way to smooth the data. In the lectures, Karl will show you some of our more advanced routines and we will see the quality of the research grade SWE products that include more difficult time/space smoothing. For now, we will explore how smoothing works with some simpler methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WY_04_05.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake the graph with a cubic spline interpolation of the point estimates from AMSR and then use a rolling average\n",
    "\n",
    "#calculate the cubic spline estimate of the AMSR data \n",
    "amsrSWE_interp=WY_04_05['SWE_mm'].interpolate(method='cubicspline')\n",
    "\n",
    "# 7 day  moving average of the interpolated data\n",
    "amsrSWE_avg=amsrSWE_interp.rolling(7).mean()\n",
    "\n",
    "\n",
    "#plot the smoothed output.\n",
    "# Create figure and plot space\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Add x-axis and y-axis\n",
    "ax.plot(amsrSWE_avg.index.values,\n",
    "        amsrSWE_avg,\n",
    "        color='purple')\n",
    "\n",
    "# Set title and labels for axes\n",
    "ax.set(xlabel=\"Date\",\n",
    "       ylabel=\"Snow Water Equivalent (mm)\",\n",
    "       title=\"Daily Snow Water Equivalent \\n\" + watershedName)\n",
    "\n",
    "# Rotate tick marks on x-axis\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ADD have students pick a water year \n",
    "#and copy/paste/edit the code they need to smooth another water year of SWE data in to the cells below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate volume of SWE in the basin each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compute_volume(dataCube,basinMask):\n",
    "    idx=np.nonzero(basinMask)\n",
    "    numValid=np.count_nonzero(basinMask)\n",
    "    numel=dataCube.shape\n",
    "    output=np.zeros(numel[0])   \n",
    "    for i in range(numel[0]):\n",
    "        daySWE=np.squeeze(dataCube[i,:,:])\n",
    "        \n",
    "        #remeber that values above 240 are fill in the original dataset, so we dont want to add - assume missing data\n",
    "        daySWE[daySWE>240]=0\n",
    "        \n",
    "        #volume of water in each pixel\n",
    "        #SWE*area=volumne of water - we want volume in cubic kilometers\n",
    "        swe2km3=(31**2)/1000000\n",
    "        daySWE=daySWE*swe2km3\n",
    "        #volume of water in the basin\n",
    "        #all data for this lab has been reporjected to 31km pixels equivalent to the ERA-5 grid.\n",
    "        #AMSR-U SWE is in 25km native resolution      \n",
    "        output[i]=daySWE[idx].sum()\n",
    "    return output\n",
    "\n",
    "dailyBasinSnowWaterVolume=compute_volume(SWEarray,basinMask)\n",
    "dailyBasinSnowWaterVolume.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a new dataframe with the new basin-wide SWE volume data included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFile=watershedName+'AMSR_sweDates.csv'\n",
    "SWEdf=pd.read_csv(dateFile,header=None,names=['sweDates'])\n",
    "SWEdf['km3 snowpack storage']=pd.DataFrame(dailyBasinSnowWaterVolume)\n",
    "SWEdf['sweDates']=pd.to_datetime(SWEdf['sweDates'],format='%Y%m%d')\n",
    "SWEdf=SWEdf.set_index('sweDates')\n",
    "SWEdf.head()\n",
    "\n",
    "# Create figure and plot space\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Add x-axis and y-axis\n",
    "ax.plot(SWEdf.index.values,\n",
    "        SWEdf['km3 snowpack storage'],\n",
    "        color='purple')\n",
    "\n",
    "# Set title and labels for axes\n",
    "ax.set(xlabel=\"Date\",\n",
    "       ylabel=\"water volume in snow (km3)\",\n",
    "       title=\"Daily water stored in the snowpack in the basin \\n\" + watershedName)\n",
    "\n",
    "# Rotate tick marks on x-axis\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at basin-wide water volume for the 2004-2005 water year again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WY_04_05=SWEdf['2004-10-01':'2005-09-30']\n",
    "# Create figure and plot space\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Add x-axis and y-axis\n",
    "ax.plot(WY_04_05.index.values,\n",
    "        WY_04_05,\n",
    "        color='purple')\n",
    "\n",
    "# Set title and labels for axes\n",
    "ax.set(xlabel=\"Date\",\n",
    "       ylabel=\"Snow Water Equivalent (km3)\",\n",
    "       title=\"Daily Snow Water Equivalent \\n\" + watershedName)\n",
    "\n",
    "# Rotate tick marks on x-axis\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can really see those interpolated values in our data which have replaced the fill values. Lets smooth these also and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#calculate the cubic spline estimate of the AMSR data \n",
    "amsrSWE_interp=WY_04_05['km3 snowpack storage'].interpolate(method='cubicspline')\n",
    "\n",
    "# 7 day  moving average of the interpolated data\n",
    "amsrSWE_avg=amsrSWE_interp.rolling(7).mean()\n",
    "\n",
    "\n",
    "#plot the smoothed output\n",
    "# Create figure and plot space\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Add x-axis and y-axis\n",
    "ax.plot(amsrSWE_avg.index.values,\n",
    "        amsrSWE_avg,\n",
    "        color='purple')\n",
    "\n",
    "# Set title and labels for axes\n",
    "ax.set(xlabel=\"Date\",\n",
    "       ylabel=\"Snow Water Equivalent (km3)\",\n",
    "       title=\"Daily Snow Water Equivalent \\n\" + watershedName)\n",
    "\n",
    "# Rotate tick marks on x-axis\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remeber:\n",
    "SWE is not streamflow. Many physical processes are occuring in the environment for example, sublimation, evaporation, transpiration, recharging groundwater, and human's consumptive use.\n",
    "These are SWE estimates. Operational products are not highly accurate over mountainous terrain.\n",
    "However, the tools and techniques that we can use to work with the data and get information from it (i.e SWE to water volume) are still useful in understanding the relative amounts of SWE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PART 2: ERA-5 SWE\n",
    "Lets look at the ERA-5 data now. \n",
    "\n",
    "add url to era-5 data, add intro text\n",
    "\n",
    "explain it will be the same operaiton as we did with the AMSR data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean up our workspace we dont need to keep some of the data above loaded anymore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load ERA-5\n",
    "\n",
    "[add intro here like for AMSR; will data not be cleared here? If so, need to redefine watershedName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ERA5 SWE as a data frame\n",
    "aoi_ERA5_SWE=watershedName+'_ERA5_SWE.tif'\n",
    "SWE=rio.open(aoi_ERA5_SWE)\n",
    "SWEarray = SWE.read()\n",
    "numel=SWEarray.shape\n",
    "print(numel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data frame of era5 + dates for one pixel (same pixel as the AMSR part of the lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load dates (and?)\n",
    "dateFile=watershedName+'ERA5_sweDates.csv'\n",
    "SWEdf=pd.read_csv(dateFile,header=None,names=['sweDates'])\n",
    "SWEdf['SWE_mm']=pd.DataFrame(SWEarray[:,4,5])#,columns=['SWE_mm'],index=)\n",
    "SWEdf['sweDates']=pd.to_datetime(SWEdf['sweDates'],format='%d-%b-%Y %H:%M:%S')\n",
    "SWEdf=SWEdf.set_index('sweDates')\n",
    "SWEdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create figure and plot space\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Add x-axis and y-axis\n",
    "ax.plot(SWEdf.index.values,\n",
    "        SWEdf['SWE_mm'],\n",
    "        color='purple')\n",
    "\n",
    "# Set title and labels for axes\n",
    "ax.set(xlabel=\"Date\",\n",
    "       ylabel=\"Snow Water Equivalent (mm)\",\n",
    "       title=\"Daily Snow Water Equivalent \\n\" + watershedName)\n",
    "\n",
    "# Rotate tick marks on x-axis\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look Different? ERA5 gives a much longer historical record of the SWE for the location. This is a nice feture of the models and a dangerous one. They can calculate values for times when they do not have much data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the 2004-2005 water year like we did with AMSR. Again, the water year starts October 1, 2004 and ends September 30, 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code here\n",
    "WY_04_05=SWEdf['2004-10-01':'2005-09-30']\n",
    "\n",
    "# Create figure and plot space\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Add x-axis and y-axis\n",
    "ax.plot(WY_04_05.index.values,\n",
    "        WY_04_05,\n",
    "        color='purple')\n",
    "\n",
    "# Set title and labels for axes\n",
    "ax.set(xlabel=\"Date\",\n",
    "       ylabel=\"Snow Water Equivalent (mm)\",\n",
    "       title=\"Daily Snow Water Equivalent \\n\" + watershedName)\n",
    "\n",
    "# Rotate tick marks on x-axis\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see there is no need to interpolate or smooth! An advantage of model outputs relative to satellite observations is that they are continous. The physical equations are used to generate estimates between measurements, this is a form of \"data assimmilation\" where the combination of models and measuremtns enable you to make the best estimate possible, considering all sources of information you have available, at any timestep.\n",
    "\n",
    "With the long history of data available from the model we can contextualize any year within the historical context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(SWEdf.index.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WY_04_05=SWEdf['2004-10-01':'2005-09-30']\n",
    "\n",
    "\n",
    "# Create figure and plot space\n",
    "#fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.figure(figsize=(15,10));\n",
    "#plot all the background years\n",
    "yrs=np.unique(SWEdf.index.year)\n",
    "numYears=len(yrs)-1\n",
    "\n",
    "for i in range(numYears):\n",
    "    sDate=str(yrs[i])+'-10-01'\n",
    "    eDate=str(yrs[i+1])+'-09-30'\n",
    "    WY=SWEdf[sDate:eDate]\n",
    "\n",
    "\n",
    "# Add x-axis and y-axis - x axis is not year this tim ebecuase we have lots of years overlaied\n",
    "    plt.plot(list(range(len(WY))),\n",
    "            WY,\n",
    "            color='grey')\n",
    "\n",
    "\n",
    "# Rotate tick marks on x-axis\n",
    "#plt.show()\n",
    "#xData=list(range(len(WY_04_05)))\n",
    "#now lets highlight the 2004-2005 water year\n",
    "\n",
    "# Add x-axis and y-axis\n",
    "plt.plot(list(range(len(WY_04_05))),\n",
    "         WY_04_05,\n",
    "         color='purple')\n",
    "\n",
    "# Set title and labels for axes\n",
    "titleText='Daily Snow Water Equivalent \\n' + watershedName\n",
    "plt.title(titleText);\n",
    "plt.xlabel(\"Water Year Days\");\n",
    "plt.ylabel(\"Snow Water Equivalent (mm)\");\n",
    "#plt.setp(ax.get_xticklabels(), rotation=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph gives some context of the highlighted year we are interested in, in the context of other years. But the graph is still kind of messy. Lets look at another way we can visualize a year of interest in the context of the historical record. What is the driest year? The wettest year? To figure this out, and plot them along with some more historical context, we will use on of the more powerful datascience functions in python the \"GroupBy\" function you can use on pandas dataframes. We can use GroupBy to find the maximum SWE of each calendar year, which we can see from the above graph, for these pixels occurs at peak SWE each spring. [add some more info about group by and some links to good online references]  First, lets use GroupBy to see what the peak SWE was each year for this pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use groupby to split the dataframe into tables for each year, and then find the maximum value in the 'SWE_mm' column\n",
    "peakSWE=SWEdf.groupby(SWEdf.index.year)['SWE_mm'].max().reset_index()\n",
    "#the data goes into february 2021, so we want to drop the 2020-2021 water year as it is not yet to peak SWE\n",
    "peakSWE=peakSWE[:-1]\n",
    "#view results\n",
    "print(peakSWE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of the peak SWE each year, we can use this information to identify the water year that had the largest snowpack and the water year that had the smallest snowpack for our contextualization graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what was the driest year?\n",
    "dryYear=peakSWE.loc[peakSWE[\"SWE_mm\"].idxmin()]['sweDates']\n",
    "print('driest year:', dryYear, 'peak SWE', peakSWE[\"SWE_mm\"].min(), 'mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#what was the wettest year? for this we use .idxmax() instead of .idxmin()\n",
    "wetYear=peakSWE.loc[peakSWE[\"SWE_mm\"].idxmax()]['sweDates']\n",
    "print('wettest year:', wetYear, 'peak SWE', peakSWE[\"SWE_mm\"].max(), 'mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numYears=[dryYear, wetYear]\n",
    "print(numYears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how different are the wettest and the driest year?\n",
    "plt.figure(figsize=(15,10));\n",
    "\n",
    "numYears=[dryYear, wetYear]\n",
    "for i in numYears:\n",
    "    sDate=str(i-1)+'-10-01'\n",
    "    eDate=str(i)+'-09-30'\n",
    "    WY=SWEdf[sDate:eDate]\n",
    "\n",
    "\n",
    "# Add x-axis and y-axis - x axis is not year this tim ebecuase we have lots of years overlaied\n",
    "    plt.plot(list(range(len(WY))),\n",
    "            WY,\n",
    "            color='grey')\n",
    "\n",
    "titleText='Daily Snow Water Equivalent \\n' + watershedName\n",
    "plt.title(titleText);\n",
    "plt.xlabel(\"Water Year Days\");\n",
    "plt.ylabel(\"Snow Water Equivalent (mm)\");\n",
    "#plt.setp(ax.get_xticklabels(), rotation=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the wettest and driest years, lets use that information to add historical context to our graph of the year of interest. In additon, we can look at the interquartile range of SWE through time. This is the bounds for any given day for which the SWE model is within the range 50% of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the interquartile range (25th percintile and 75th percintile of SWE from ERA-5 for this pixel \n",
    "#for each day of year across the record. \n",
    "DOWY=np.arange(366)\n",
    "SWE_q1=np.zeros(len(DOWY))\n",
    "SWE_q3=np.zeros(len(DOWY))\n",
    "\n",
    "for i in range(0,365):\n",
    "    #get the data for the day of the year for each year in the record\n",
    "    temp=SWEdf[SWEdf.index.dayofyear==i+1]\n",
    "    \n",
    "    #figure out which day of the water year the day of the year is (not accounting for leap year) \n",
    "    #- could make this a second step in the plot\n",
    "    if i>=274:\n",
    "        k=i-274\n",
    "    else:\n",
    "        k=i+(365-274)\n",
    "  \n",
    "    #fill in the correct day of water year location in the arrays for ploting\n",
    "    SWE_q1[k]=np.percentile(temp['SWE_mm'],25,interpolation='midpoint')\n",
    "    SWE_q3[k]=np.percentile(temp['SWE_mm'],75,interpolation='midpoint')\n",
    "\n",
    "#plot the wettest year, the driest year, the interquartile range, and the year of interest on a single graph\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(15,10))\n",
    "ax.fill_between(DOWY,SWE_q1,SWE_q3)\n",
    "\n",
    "#add the wet and dry year for context\n",
    "numYears=[dryYear, wetYear]\n",
    "for i in numYears:\n",
    "    sDate=str(i-1)+'-10-01'\n",
    "    eDate=str(i)+'-09-30'\n",
    "    WY=SWEdf[sDate:eDate]\n",
    "\n",
    "\n",
    "# Add x-axis and y-axis - x axis is not year this tim ebecuase we have lots of years overlaied\n",
    "    ax.plot(list(range(len(WY))),\n",
    "            WY,\n",
    "            color='red')\n",
    "\n",
    "    # Add x-axis and y-axis\n",
    "plt.plot(list(range(len(WY_04_05))),\n",
    "         WY_04_05,\n",
    "         color='black')\n",
    "    \n",
    "    \n",
    "titleText='Daily Snow Water Equivalent \\n' + watershedName\n",
    "plt.title(titleText);\n",
    "plt.xlabel(\"Water Year Days\");\n",
    "plt.ylabel(\"Snow Water Equivalent (mm)\");\n",
    "#plt.setp(ax.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting, dicuss with fellow students how you would interpret this graph. Is the wettest year always the most swe on a given day for a pixel? Is the driest year always the driest? \n",
    "lets now calcuate basin wide storage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
